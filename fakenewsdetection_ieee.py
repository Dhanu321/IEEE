# -*- coding: utf-8 -*-
"""Fakenewsdetection IEEE

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1z_YQYrJ3NwO_esxUXo0igpQhO1Qx0amy
"""

from google.colab import files
import pandas as pd

# Upload the file manually
uploaded = files.upload()

# Load the uploaded file into a Pandas DataFrame
for filename in uploaded.keys():
    print(f"Uploaded file: {filename}")
    df = pd.read_csv(filename)  # Change to read_excel(filename) if using an Excel file

# Display the first few rows
df.head()

from google.colab import files
import pandas as pd

# Upload the file manually
uploaded = files.upload()

# Load the uploaded file into a Pandas DataFrame
for filename in uploaded.keys():
    print(f"Uploaded file: {filename}")
    df = pd.read_csv(filename)  # Change to read_excel(filename) if using an Excel file

# Display the first few rows
df.head()

from google.colab import files
import pandas as pd

# Upload the file manually
uploaded = files.upload()

# Load the uploaded file into a Pandas DataFrame
for filename in uploaded.keys():
    print(f"Uploaded file: {filename}")
    df = pd.read_csv(filename)  # Change to read_excel(filename) if using an Excel file

# Display the first few rows
df.head()

import pandas as pd

# List of dataset filenames (Update these based on your uploaded files)
filenames = ["Users.csv", "profile_features.csv", "statistical_features.csv", "textual_features.csv", "emotional_features.csv"]

# Check each dataset
for file in filenames:
    try:
        df = pd.read_csv(file)
        print(f"ðŸ”¹ Dataset: {file}")
        print(df.head())  # Show first few rows
        print("\nColumns:\n", df.columns)  # Show column names
        print("\nMissing Values:\n", df.isnull().sum())  # Check missing values
        print("\n-----------------------------\n")
    except Exception as e:
        print(f"Error loading {file}: {e}")

import pandas as pd

# Load datasets
users = pd.read_csv("Users.csv")
statistical = pd.read_csv("statistical_features.csv")
textual = pd.read_csv("textual_features.csv")
emotional = pd.read_csv("emotional_features.csv")

# Merge all required datasets on 'user_id'
df = users.merge(statistical, on="user_id").merge(textual, on="user_id").merge(emotional, on="user_id")

# Remove 'user_id' (as itâ€™s just an identifier)
df.drop(columns=['user_id'], inplace=True)

# Convert N_False and N_True into a single binary label
df['label'] = df['N_True'].apply(lambda x: 1 if x > 0 else 0)
df.drop(columns=['N_False', 'N_True'], inplace=True)

# Display the final dataset structure
print("ðŸ”¹ Final Dataset Structure:\n", df.head())
print("\nðŸ”¹ Missing Values:\n", df.isnull().sum())

import pandas as pd

# Load datasets
users = pd.read_csv("Users.csv")
statistical = pd.read_csv("statistical_features.csv")  # Keep all columns
textual = pd.read_csv("textual_features.csv")  # Keep only 'user_id' & 'tri_grams'
emotional = pd.read_csv("emotional_features.csv")  # Keep all columns

# ðŸ”¹ Remove Profile Features (Not Needed)
# Do NOT load 'profile_features.csv' since we are removing profile-related data

# Merge datasets (on 'user_id')
df = users.merge(statistical, on="user_id").merge(textual, on="user_id").merge(emotional, on="user_id")

# Drop 'user_id' (only used for merging)
df.drop(columns=['user_id'], inplace=True)

# ðŸ”¹ Convert N_False and N_True into a single binary label
df['label'] = df['N_True'].apply(lambda x: 1 if x > 0 else 0)
df.drop(columns=['N_False', 'N_True'], inplace=True)

# âœ… Check Final Structure
print("âœ… Data Merged Successfully! Shape:", df.shape)
print("ðŸ”¹ Columns in Final DataFrame:\n", df.columns)

import seaborn as sns
import matplotlib.pyplot as plt

# ðŸ”¹ Check summary statistics
print("âœ… Summary Statistics:\n", df.describe())

# ðŸ”¹ Plot class distribution
plt.figure(figsize=(5, 4))
sns.countplot(x=df['label'])
plt.title("Class Distribution")
plt.show()

# ðŸ”¹ Print class counts
print("ðŸ”¹ Label Counts:\n", df['label'].value_counts())

# ðŸ”¹ Histogram of numerical features
df.drop(columns=['label']).hist(figsize=(15, 12), bins=30)
plt.suptitle("Feature Distributions")
plt.show()

import ast
import numpy as np

# Convert `tri_grams` from string representation to actual lists
df['tri_grams'] = df['tri_grams'].apply(lambda x: np.array(ast.literal_eval(x)) if isinstance(x, str) else np.array(x))

# Standardize the length of `tri_grams`
MAX_FEATURES = 300  # Choose a fixed number of features for PCA
df['tri_grams'] = df['tri_grams'].apply(lambda x: np.pad(x[:MAX_FEATURES], (0, max(0, MAX_FEATURES - len(x))), mode='constant'))

# Convert `tri_grams` array into separate columns
tri_grams_df = pd.DataFrame(df['tri_grams'].tolist(), columns=[f'tri_gram_{i}' for i in range(MAX_FEATURES)])

# Remove old `tri_grams` column and merge with the main dataset
df = df.drop(columns=['tri_grams']).reset_index(drop=True)
df = pd.concat([df, tri_grams_df], axis=1)

print("âœ… `tri_grams` processed successfully! New shape:", df.shape)

import seaborn as sns
import matplotlib.pyplot as plt

# ðŸ”¹ Plot the correlation heatmap
plt.figure(figsize=(12, 6))
sns.heatmap(df.corr(), cmap='coolwarm', annot=False)
plt.title("Feature Correlation Heatmap")
plt.show()

profile_features = [
    'statuses_count', 'followers_count', 'following_count', 'favourites_count',
    'listed_count', 'default_profile', 'verified', 'tweet_freq',
    'follower_growth_rate', 'following_growth_rate', 'listed_growth_rate',
    'followers_following_ratio', 'screen_name_length', 'digits_in_screen_name',
    'name_length', 'digits_in_name', 'description_length'
]

# Drop these columns
df = df.drop(columns=profile_features, errors='ignore')

print("âœ… Profile-related features removed!")
print("ðŸ”¹ Remaining columns:", df.columns)

print("Available Columns in df:\n", df.columns)

print(df.head())  # Check first few rows

print("Columns in df:", df.columns)
print("Columns in df_textual:", df_textual.columns)

df_users = pd.read_csv("Users.csv")  # Load original user dataset

# Merge `user_id` back into df
df = df_users[['user_id']].merge(df, left_index=True, right_index=True)

print("âœ… `user_id` restored! Columns in df now:", df.columns)

df = df.merge(df_textual[['user_id', 'tri_grams']], on="user_id", how="left")
print("âœ… `tri_grams` column merged successfully!")

print("Missing Values in `tri_grams`:\n", df['tri_grams'].isnull().sum())

# Import necessary library
import numpy as np

# Define a function to replace missing `tri_grams` values with zeros
def fix_missing_tri_grams(x):
    if isinstance(x, str):  # Convert from string if needed
        return np.array(ast.literal_eval(x))
    elif isinstance(x, np.ndarray):  # If already a NumPy array, keep it
        return x
    else:  # If missing, replace with zeros of appropriate length
        return np.zeros(42939)  # Adjust size based on dataset

# Apply the function
df['tri_grams'] = df['tri_grams'].apply(fix_missing_tri_grams)

# Check again if there are missing values
print("âœ… Missing values fixed! Remaining missing:", df['tri_grams'].isnull().sum())

import numpy as np
import ast

# Convert `tri_grams` safely to lists
def safe_convert(x):
    """Convert string lists to actual lists and handle NaN values."""
    try:
        if isinstance(x, str):
            return np.array(ast.literal_eval(x))
        elif isinstance(x, (list, np.ndarray)):
            return np.array(x)
        else:
            return np.array([])  # Default to empty array
    except (ValueError, SyntaxError, TypeError):
        return np.array([])  # Handle errors safely

df['tri_grams'] = df['tri_grams'].apply(safe_convert)

# Handle missing values: Replace empty lists with zero-padded arrays
fixed_size = 1000  # Adjust as needed

def fix_length(arr, size=fixed_size):
    """Ensure all arrays have the same length by padding or truncating."""
    arr = np.array(arr, dtype=float)  # Convert to NumPy array
    if arr.size == 0:
        return np.zeros(size)  # If empty, return zero vector
    elif len(arr) > size:
        return arr[:size]  # Truncate if too long
    else:
        return np.pad(arr, (0, size - len(arr)))  # Pad with zeros

df['tri_grams_fixed'] = df['tri_grams'].apply(fix_length)

# Convert `tri_grams_fixed` into a NumPy array
tri_grams_array = np.vstack(df['tri_grams_fixed'].values)

print(f"âœ… All `tri_grams` vectors are now of size {fixed_size}!")

# Apply PCA to reduce to 300 dimensions
from sklearn.decomposition import PCA

pca = PCA(n_components=300, random_state=42)
tri_grams_reduced = pca.fit_transform(tri_grams_array)

# Convert PCA results back to DataFrame
tri_grams_df = pd.DataFrame(tri_grams_reduced, columns=[f"tri_gram_{i}" for i in range(300)])

# Drop old `tri_grams` columns & merge PCA results
df = df.drop(columns=['tri_grams', 'tri_grams_fixed']).reset_index(drop=True)
df = pd.concat([df, tri_grams_df], axis=1)

print(f"âœ… PCA Applied! Reduced `tri_grams` to {tri_grams_df.shape[1]} dimensions.")

# Check final dataset shape
print("âœ… Final Data Shape:", df.shape)

# Check if there are any missing values
print("ðŸ”¹ Missing Values:\n", df.isnull().sum())

# Display first few rows
df.head()

import matplotlib.pyplot as plt
import seaborn as sns

# Select some important numerical features
selected_features = ['non_duplicate', 'hashtags_prop', 'mentions_prop', 'urls_prop', 'count_RT', 'avg_engagement']

# Plot histograms
df[selected_features].hist(figsize=(12, 8), bins=30)
plt.suptitle("Univariate Analysis: Feature Distributions")
plt.show()

# ðŸ”¹ Correlation heatmap
plt.figure(figsize=(12, 6))
sns.heatmap(df[selected_features + ['label']].corr(), cmap='coolwarm', annot=True, fmt=".2f")
plt.title("Feature Correlation Heatmap")
plt.show()

# Scatter plots for key features
for feature in selected_features:
    plt.figure(figsize=(5, 4))
    sns.scatterplot(x=df[feature], y=df['label'], alpha=0.5)
    plt.title(f"{feature} vs Label")
    plt.show()

from sklearn.decomposition import PCA

# Apply PCA to reduce dimensionality to 2 components
pca = PCA(n_components=2, random_state=42)
X_pca = pca.fit_transform(df.drop(columns=['user_id', 'label']))

# Plot PCA result
plt.figure(figsize=(8, 6))
sns.scatterplot(x=X_pca[:, 0], y=X_pca[:, 1], hue=df['label'], palette="coolwarm", alpha=0.6)
plt.title("PCA Visualization (2D)")
plt.xlabel("Principal Component 1")
plt.ylabel("Principal Component 2")
plt.show()

import seaborn as sns

# Select key features for pairplot
selected_features = ['hashtags_prop', 'n_unique_mentions', 'count_RT', 'avg_engagement', 'anger', 'optimism', 'sadness']

# Create pairplot
sns.pairplot(df[selected_features + ['label']], hue="label", palette="coolwarm")
plt.show()

print("ðŸ”¹ Available Columns in df:\n", df.columns)

# Reload the original textual dataset
df_textual = pd.read_csv("textual_features.csv")

# Merge back the 'tri_grams' column
df = df.merge(df_textual[['user_id', 'tri_grams']], on="user_id", how="left")

print("âœ… `tri_grams` column restored!")
print("Available Columns:\n", df.columns)

import nltk

# Manually install only 'punkt' again
nltk.data.find('tokenizers/punkt')
print("âœ… 'punkt' is already installed!")

nltk.download('stopwords')
from nltk.corpus import stopwords
stop_words = set(stopwords.words('english'))

print("âœ… Stopwords Loaded Successfully!")

import os
os.environ['NLTK_DATA'] = '/root/nltk_data'

print("âœ… Set NLTK data path manually!")

import os

print("ðŸ”¹ punkt Exists:", os.path.exists('/root/nltk_data/tokenizers/punkt'))
print("ðŸ”¹ stopwords Exists:", os.path.exists('/root/nltk_data/corpora/stopwords'))

import importlib
import nltk

importlib.reload(nltk)
print("âœ… NLTK Reloaded Successfully!")

import nltk
import shutil
import os

# Delete corrupted or outdated tokenizers
shutil.rmtree('/root/nltk_data/tokenizers', ignore_errors=True)

# Manually set NLTK data path
nltk.data.path.append('/root/nltk_data')

# Re-download necessary NLTK components
nltk.download('punkt')
nltk.download('stopwords')

print("âœ… NLTK Tokenizers Reset and Reinstalled!")

import nltk
nltk.data.path.append("/usr/local/lib/nltk_data")

import nltk
nltk.download("punkt")
nltk.download("stopwords")
nltk.download("omw-1.4")  # Ensure WordNet is downloaded (if needed)

import shutil
shutil.rmtree("/root/nltk_data/tokenizers", ignore_errors=True)

import nltk
nltk.download("punkt")

import shutil
shutil.rmtree('/root/nltk_data/tokenizers', ignore_errors=True)
print("âœ… Deleted old tokenizers folder!")

import nltk
nltk.download('punkt')
nltk.download('stopwords')
print("âœ… Downloaded `punkt` and `stopwords` successfully!")

import os
nltk.data.path.append("/root/nltk_data")
print("âœ… Set NLTK data path manually!")

print("ðŸ”¹ punkt Exists:", os.path.exists('/root/nltk_data/tokenizers/punkt'))
print("ðŸ”¹ stopwords Exists:", os.path.exists('/root/nltk_data/corpora/stopwords'))

import shutil
shutil.rmtree('/root/nltk_data', ignore_errors=True)
print("âœ… Deleted NLTK Data Successfully!")